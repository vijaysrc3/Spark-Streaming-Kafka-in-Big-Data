{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='text-aling:center;color:Navy'>  Big Data Systems - Fall 2021  </h1>\n",
    "<h1 style='text-aling:center;color:Navy'>  Assignment 6 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is focused on Spark Streaming and Kafka.\n",
    "\n",
    "To complete the assignment, you should complete this notebook by filling in the cells provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Submission Deadline: This assignment is due Wednesday, Nov 17 at 11:59 P.M</b>\n",
    "\n",
    "<div style=\"font-size:26px;color:#F1F8FC;background-color:red;padding:20px;line-height:30px\">Please run the notebook on your local machine but do copy and paste your solution here and submit to Okpy</div>\n",
    "\n",
    "A few notes before you start:\n",
    "- Directly sharing answers is not okay, but discussing problems with other students is encouraged.\n",
    "- You should start early so that you have time to get help if you're stuck.\n",
    "\n",
    "- Before continuing the assignment, select \"Save and Checkpoint\" in the File menu and then execute the submit cell below. The result will contain a link that you can use to check that your assignment has been submitted successfully. If you submit more than once before the deadline, we will only grade your final submission. If you mistakenly submit the wrong one, you can head to okpy.org and flag the correct version. There will be another submit cell at the end of the assignment when you finish!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid orange; margin-top: 1px; margin-bottom: 1px\"></hr>\n",
    "<br>\n",
    "Before you begin completing the assignment, execute the following cell to load the provided tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "Assignment: Assignment6: Big Stream Data Processing Systems\n",
      "OK, version v1.15.44\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Don't change this cell; just run it. \n",
    "# When you log-in please hit return (not shift + return) after typing in your email\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('Assignment6.ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_notebook();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving notebook... Saved 'Assignment6.ipynb'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit... 0.0% complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit... 100% complete\n",
      "Submission successful for user: vijayasimha.bheemireddy@ucdenver.edu\n",
      "URL: https://kubemaster.ucdenver.pvt:5000/UCDenver/CSCI4951-5951/fa21/assignment6/submissions/e98EYd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = ok.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid orange; margin-top: 1px; margin-bottom: 1px\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid purple; margin-top: 1px; margin-bottom: 1px\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:26px;color:#F1F8FC;background-color:#0095EA;padding:10px;\">Exersice 1 - Tweet analysis </div><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start with this part of the assignment, we will need to run the python program *tweetread.py* that reads the tweets and streams those to a local port.\n",
    "\n",
    "On a text editor open the file *tweetread.py*. We need to configure the following parameters. You should obtain the value for each parameter from your tweeter configuration (review the guide).\n",
    "\n",
    "<pre style=\"background-color: #ebece4;padding: 10px;border-left: solid 4px orange;\">\n",
    "# Set up your credentials\n",
    "# add your credentials between ''\n",
    "\n",
    "consumer_key='1QPpi89KXVAiOul69pqeQucsl'\n",
    "consumer_secret='tKUlev4ks3W5pOHCBDg02BOggU3yXLvqImXjkzL5eiSIXE0rQa'\n",
    "access_token ='1447305213321236480-qYqlJzfmswlJS08ffdBPAas7pjqxSQ'\n",
    "access_secret='enQen0nmzJTpP5ulxsfMmYMWZ9kutevCuhN14hJ2ZG8aH'\n",
    "\n",
    "</pre>\n",
    "\n",
    "\n",
    "Also, you might need to change this port in case you receive a *'port xxxx is already in use'* error.\n",
    "<pre style=\"background-color: #ebece4;padding: 10px;border-left: solid 4px orange;\">\n",
    "port = 5555\n",
    "</pre>\n",
    "\n",
    "\n",
    "**Open a new shell terminal** and execute the following commands.<br>\n",
    "*We assume you have your downloaded files for this assignment at **~/assignment**, if you are using another path, replace it below.*\n",
    "<pre style=\"background-color: #ebece4;padding: 10px;border-left: solid 4px orange;\">\n",
    "cd ~/assignment <br>\n",
    "python3 tweetread.py\n",
    "</pre>\n",
    "\n",
    "Once you have this running, return to the notebook and keep going. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the spark path in the folloiwng cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('-- Type the path of your spark foler -- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May cause deprecation warnings, safe to ignore, they aren't errors\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalTweetCount = 0 #stores the number of tweets\n",
    "totalCharacters = 0 #stores the total length of all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can only run this once. restart your kernel for any errors.\n",
    "#to do so click on Kernel, Resrart & Clear Output\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Notes**: \n",
    "    - If you change the port number on tweetread.py, you need to update it here too.<br>\n",
    "    - Both port numbers (here and tweetread.py) should match. <br>\n",
    "    - You might need to restart the Kernel before continuing.<br>\n",
    "    - **Make sure your *tweetread.py* app is running before continue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ssc.socketTextStream(\"127.0.0.1\", 5555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = tweets.map(lambda tweet: len(tweet)) #get the length of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateCounts(rdd):\n",
    "    try:\n",
    "        count = rdd.count()\n",
    "        if(count>0):\n",
    "            global totalTweetCount\n",
    "            global totalCharacters\n",
    "            \n",
    "            totalTweetCount += count\n",
    "            totalCharacters += rdd.reduce(lambda len1,len2: len1+len2)\n",
    "            \n",
    "        if (totalTweetCount>0):\n",
    "            print('Total Tweets: ' + str(totalTweetCount) + '\\n' +\n",
    "                  'Total characters: ' + str(totalCharacters) + '\\n'\n",
    "                  'Average Length: ' + str(totalCharacters/totalTweetCount)+\n",
    "                 '\\n------------------------------\\n')\n",
    "        else:\n",
    "            print('nothing yet')\n",
    "        \n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths.foreachRDD(lambda rdd: updateCounts(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now start the spark streaming context by running the next cell.**\n",
    "- You should see an updated output every 5 seconds\n",
    "\n",
    "*Once you are done, execute the stop command in the cell after.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:26px;color:#F1F8FC;background-color:#0095EA;padding:10px;\">Exersice 2 - Tweet hashtags analysis </div><br>\n",
    "In the following cells (add more cells need be), write code that outputs the 10 most popular hashtags in the news category. The tweetread.py filters tweets in the news category for you, so don't worry about that part.\n",
    "\n",
    "### Use the following configuration for this part of the assignment:\n",
    "- **Batch size:** 5 seconds\n",
    "- **Window interval:** 300 seconds (5 minutes)\n",
    "- **Slide interval:** 10 seconds \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:26px;color:#F1F8FC;background-color:#0095EA;padding:10px;\">Exersice 3 - Kafka </div><br>\n",
    "Go through the assignment guide for Kafka (see the pdf file in the assignment package). Once you are done reviewing the guide, complete this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:20px;background-color:#0B6713;color:#F1E6E7;padding:10px;\">\n",
    "    To-Do Number 1:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create 2 more consumers by right-clicking on the Consumer class and running it as we did before. \n",
    "- Stop the producer and run it again. \n",
    "- **Do consumers share the messages? Explain why?**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer Here.\n",
    "\n",
    "The Consumers don't share messages. When one producer and three consumers are executed, the existed one partition is alloted to anyone of the three consumers which displays the messages that are sent by producer, i.e one of the three consumers will display all the messages."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Paste the modified Producer Class Here. \n",
    "\n",
    "import org.apache.kafka.clients.producer.KafkaProducer;\n",
    "import org.apache.kafka.clients.producer.ProducerRecord;\n",
    "\n",
    "import java.util.Properties;\n",
    "import java.time.Instant;\n",
    "public class Producer {\n",
    "\n",
    "    public static void main(String[] args){\n",
    "        Properties properties = new Properties();\n",
    "        properties.put(\"bootstrap.servers\", \"localhost:9092\");\n",
    "        properties.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n",
    "        properties.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n",
    "\t\t\n",
    "        KafkaProducer<String, String> kafkaProducer = new KafkaProducer<String, String>(properties);\n",
    "        try{\n",
    "            long currentTimestamp;\n",
    "            for(int i = 0; i < 100; i++){\n",
    "                kafkaProducer.send(new ProducerRecord<String, String>(\"topictest\", Integer.toString(i), \"test message - \" + i ));\n",
    "            }\n",
    "        }catch (Exception e){\n",
    "            e.printStackTrace();\n",
    "        }finally {\n",
    "            kafkaProducer.close();\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Paste the modified Consumer Class Here\n",
    "\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecord;\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecords;\n",
    "import org.apache.kafka.clients.consumer.KafkaConsumer;\n",
    "\n",
    "import java.time.Instant;\n",
    "import java.util.ArrayList;\n",
    "import java.util.List;\n",
    "import java.util.Properties;\n",
    "import java.time.Instant;\n",
    "\n",
    "public class Consumer {\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        Properties properties = new Properties();\n",
    "        properties.put(\"bootstrap.servers\", \"localhost:9092\");\n",
    "        properties.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n",
    "        properties.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n",
    "        properties.put(\"group.id\", \"test-group\");\n",
    "\n",
    "        KafkaConsumer<String, String> kafkaConsumer = new KafkaConsumer<String, String>(properties);\n",
    "        List<String> topics = new ArrayList<String>();\n",
    "        topics.add(\"topictest\");\n",
    "        kafkaConsumer.subscribe(topics);\n",
    "        long currentTimestamp;\n",
    "        try{\n",
    "            while (true){\n",
    "                ConsumerRecords<String, String> records = kafkaConsumer.poll(10);\n",
    "                for (ConsumerRecord<String, String> record: records){\n",
    "                    currentTimestamp = Instant.now().toEpochMilli();\n",
    "                    System.out.println(\"Key: \" + record.key());\n",
    "                    System.out.println(\"Value: Message # \" + record.value());\n",
    "                    System.out.println(\"Value: Timestamp: \" + currentTimestamp);\n",
    "                    System.out.println(\"Offset: \" + record.offset());\n",
    "                    System.out.println(\"Topic: \" + record.topic());\n",
    "                    System.out.println();\n",
    "                    System.out.println();\n",
    "                    System.out.println(\"-----------------------\");\n",
    "                }\n",
    "            }\n",
    "        }catch (Exception e){\n",
    "            System.out.println(e.getMessage());\n",
    "        }finally {\n",
    "            kafkaConsumer.close();\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:20px;background-color:#0B6713;color:#F1E6E7;padding:10px;\">\n",
    "    To-Do Number 2:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a new topic (name it topicnew) with replication factor 1 and 3 partitions. \n",
    "- Modify the Consumer and Producer classes to read/send messages from/to the *topicnew* rather than the topictest.\n",
    "- Again, create 3 consumers (run the consumer class 3 times without stopping any of them) and run the producer. \n",
    "- **Do consumers share the messages? Explain why?**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer Here.\n",
    "\n",
    "The Consumers share the messages. When one producer and three consumers are executed, the existed three partitions are alloted all the three consumers which shares and displays the messages that are sent by producer."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Paste the modified Producer Class Here\n",
    "\n",
    "import org.apache.kafka.clients.producer.KafkaProducer;\n",
    "import org.apache.kafka.clients.producer.ProducerRecord;\n",
    "\n",
    "import java.util.Properties;\n",
    "import java.time.Instant;\n",
    "public class Producer {\n",
    "\n",
    "    public static void main(String[] args){\n",
    "        Properties properties = new Properties();\n",
    "        properties.put(\"bootstrap.servers\", \"localhost:9092\");\n",
    "        properties.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n",
    "        properties.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n",
    "\t\t\n",
    "        KafkaProducer<String, String> kafkaProducer = new KafkaProducer<String, String>(properties);\n",
    "        try{\n",
    "            long currentTimestamp;\n",
    "            for(int i = 0; i < 100; i++){\n",
    "                kafkaProducer.send(new ProducerRecord<String, String>(\"topicnew\", Integer.toString(i), \"test message - \" + i ));\n",
    "            }\n",
    "        }catch (Exception e){\n",
    "            e.printStackTrace();\n",
    "        }finally {\n",
    "            kafkaProducer.close();\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Paste the modified Consumer Class Here\n",
    "\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecord;\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecords;\n",
    "import org.apache.kafka.clients.consumer.KafkaConsumer;\n",
    "\n",
    "import java.time.Instant;\n",
    "import java.util.ArrayList;\n",
    "import java.util.List;\n",
    "import java.util.Properties;\n",
    "import java.time.Instant;\n",
    "\n",
    "public class Consumer {\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        Properties properties = new Properties();\n",
    "        properties.put(\"bootstrap.servers\", \"localhost:9092\");\n",
    "        properties.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n",
    "        properties.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n",
    "        properties.put(\"group.id\", \"test-group\");\n",
    "\n",
    "        KafkaConsumer<String, String> kafkaConsumer = new KafkaConsumer<String, String>(properties);\n",
    "        List<String> topics = new ArrayList<String>();\n",
    "        topics.add(\"topicnew\");\n",
    "        kafkaConsumer.subscribe(topics);\n",
    "        long currentTimestamp;\n",
    "        try{\n",
    "            while (true){\n",
    "                ConsumerRecords<String, String> records = kafkaConsumer.poll(10);\n",
    "                for (ConsumerRecord<String, String> record: records){\n",
    "                    currentTimestamp = Instant.now().toEpochMilli();\n",
    "                    System.out.println(\"Key: \" + record.key());\n",
    "                    System.out.println(\"Value: Message # \" + record.value());\n",
    "                    System.out.println(\"Value: Timestamp: \" + currentTimestamp);\n",
    "                    System.out.println(\"Offset: \" + record.offset());\n",
    "                    System.out.println(\"Topic: \" + record.topic());\n",
    "                    System.out.println();\n",
    "                    System.out.println();\n",
    "                    System.out.println(\"-----------------------\");\n",
    "                }\n",
    "            }\n",
    "        }catch (Exception e){\n",
    "            System.out.println(e.getMessage());\n",
    "        }finally {\n",
    "            kafkaConsumer.close();\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:20px;background-color:#0B6713;color:#F1E6E7;padding:10px;\">\n",
    "    To-Do Number 3:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kill one of the consumers and run the producer again. \n",
    "- **What will happen?**\n",
    "- **Do the two active consumers receive all 10 messages?**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer Here.\n",
    "\n",
    "On killing one among the three consumers the kafka reassigns the the partition that is assigned to the dead consumer to any of the other two active consumers. Therefore, the Consumers that are active i.e. the two consumers will share the messages and receive all the messages. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer Here.\n",
    "\n",
    "The two active consumers will receive all the messages that sent by producer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
